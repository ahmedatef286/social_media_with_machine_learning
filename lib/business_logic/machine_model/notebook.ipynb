{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('labeled_data.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0            0\n",
       "count                 0\n",
       "hate_speech           0\n",
       "offensive_language    0\n",
       "neither               0\n",
       "class                 0\n",
       "tweet                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check null values\n",
    "df.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  \\\n",
      "0      rt  mayasolovely  as a woman you shouldn t...   \n",
      "1        rt  mleew17  boy dats cold   tyga dwn ba...   \n",
      "2          rt  urkindofbrand dawg     rt  80sbaby...   \n",
      "3            rt  c g anderson   viva based she lo...   \n",
      "4                rt  shenikaroberts  the shit you...   \n",
      "5                      t madison x  the shit just...   \n",
      "6            brighterdays  i can not just sit up ...   \n",
      "7        8220  selfiequeenbri  cause i m tired of...   \n",
      "8     amp  you might not get ya bitch back  amp  ...   \n",
      "9     rhythmixx   hobbies include  fighting maria...   \n",
      "\n",
      "                                    processed_tweets  \n",
      "0  mayasolov woman complain clean hous amp man al...  \n",
      "1  mleew boy dat cold tyga dwn bad cuffin dat hoe...  \n",
      "2  urkindofbrand dawg sbabi life ever fuck bitch ...  \n",
      "3            c g anderson viva base look like tranni  \n",
      "4  shenikarobert shit hear might true might faker...  \n",
      "5  madison x shit blow claim faith somebodi still...  \n",
      "6  brighterday sit hate anoth bitch got much shit go  \n",
      "7  selfiequeenbri caus tire big bitch come us ski...  \n",
      "8               amp might get ya bitch back amp that  \n",
      "9          rhythmixx hobbi includ fight mariam bitch  \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "#extending the stopwords to include other words used in twitter such as retweet(rt) etc.\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\" , \"RT\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(tweet):  \n",
    "    # removal of extra spaces\n",
    "    regex_pat = re.compile(r'\\s+')\n",
    "    tweet_space = tweet.str.replace(regex_pat, ' ' , regex=True)\n",
    "\n",
    "    # removal of @name[mention]\n",
    "    regex_pat = re.compile(r'@[\\w\\-]+')\n",
    "    tweet_name = tweet_space.str.replace(regex_pat, '' , regex=True)\n",
    "\n",
    "    # removal of links[https://abc.com]\n",
    "    giant_url_regex =  re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "            r'[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    tweets = tweet_name.str.replace(giant_url_regex, '', regex=True)\n",
    "    \n",
    "    # removal of punctuations and numbers\n",
    "    punc_remove = tweets.str.replace(r\"[^a-zA-Z]\", \" \", regex=True)\n",
    "    # remove whitespace with a single space\n",
    "    newtweet=punc_remove.str.replace(r'\\s+', ' ', regex=True)\n",
    "    # remove leading and trailing whitespace\n",
    "    newtweet=newtweet.str.replace(r'^\\s+|\\s+?$','', regex=True)\n",
    "    # replace normal numbers with numbr\n",
    "    newtweet=newtweet.str.replace(r'\\d+(\\.\\d+)?','numbr', regex=True)\n",
    "    # removal of capitalization\n",
    "    tweet_lower = newtweet.str.lower()\n",
    "    \n",
    "    # tokenizing\n",
    "    tokenized_tweet = tweet_lower.apply(lambda x: x.split())\n",
    "    \n",
    "    # removal of stopwords\n",
    "    tokenized_tweet=  tokenized_tweet.apply(lambda x: [item for item in x if item not in stopwords])\n",
    "    \n",
    "    # stemming of the tweets\n",
    "    tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) \n",
    "    \n",
    "    for i in range(len(tokenized_tweet)):\n",
    "        tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "        tweets_p= tokenized_tweet\n",
    "    \n",
    "    return tweets_p\n",
    "    \n",
    "processed_tweets = preprocess(df.tweet)   \n",
    "\n",
    "df['processed_tweets'] = processed_tweets\n",
    "print(df[[\"tweet\",\"processed_tweets\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9987895904781118\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       290\n",
      "           1       1.00      1.00      1.00      3832\n",
      "           2       1.00      1.00      1.00       835\n",
      "\n",
      "    accuracy                           1.00      4957\n",
      "   macro avg       1.00      0.99      1.00      4957\n",
      "weighted avg       1.00      1.00      1.00      4957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assume 'df' is your DataFrame with the dataset\n",
    "X = df[['tweet', 'count', 'offensive_language', 'hate_speech' , 'neither']]\n",
    "y = df['class']\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(lambda x: x.lower())  # Convert to lowercase\n",
    "df['tweet'] = df['tweet'].replace('[^a-zA-Z0-9]', ' ', regex=True)  # Remove special characters\n",
    "\n",
    "# Convert text to numerical features using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['tweet'])\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val['tweet'])\n",
    "\n",
    "# Concatenate TF-IDF features with other numerical features\n",
    "X_train_final = X_train[['count', 'offensive_language', 'hate_speech','neither']].values\n",
    "X_train_final = hstack([X_train_tfidf, X_train_final])\n",
    "X_val_final = X_val[['count', 'offensive_language', 'hate_speech','neither']].values\n",
    "X_val_final = hstack([X_val_tfidf, X_val_final])\n",
    "\n",
    "# Train a model (e.g., RandomForestClassifier)\n",
    "clf =  RandomForestClassifier()\n",
    "clf.fit(X_train_final, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = clf.predict(X_val_final)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for the sentence is: 0\n"
     ]
    }
   ],
   "source": [
    "# Example sentence\n",
    "new_sentence ='i hate women'\n",
    "\n",
    "# Preprocess the new sentence\n",
    "new_sentence = new_sentence.lower()\n",
    "new_sentence = pd.Series(new_sentence).replace('[^a-zA-Z0-9]', ' ', regex=True)[0]\n",
    "\n",
    "# Convert the new sentence to TF-IDF features\n",
    "new_sentence_tfidf = tfidf_vectorizer.transform([new_sentence])\n",
    "\n",
    "# Add other numerical features if needed\n",
    "# Example:\n",
    "new_numerical_features = [0, 0, 0, 0]  # Replace with your own numerical features\n",
    "new_sentence_final = hstack([new_sentence_tfidf, new_numerical_features])\n",
    "\n",
    "# Make predictions for the new sentence\n",
    "prediction = clf.predict(new_sentence_final)\n",
    "\n",
    "# Print the prediction\n",
    "print(f\"The prediction for the sentence is: {prediction[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
