{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('labeled_data.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0            0\n",
       "count                 0\n",
       "hate_speech           0\n",
       "offensive_language    0\n",
       "neither               0\n",
       "class                 0\n",
       "tweet                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check null values\n",
    "df.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  \\\n",
      "0  !!! RT @mayasolovely: As a woman you shouldn't...   \n",
      "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
      "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
      "5  !!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...   \n",
      "6  !!!!!!\"@__BrighterDays: I can not just sit up ...   \n",
      "7  !!!!&#8220;@selfiequeenbri: cause I'm tired of...   \n",
      "8  \" &amp; you might not get ya bitch back &amp; ...   \n",
      "9  \" @rhythmixx_ :hobbies include: fighting Maria...   \n",
      "\n",
      "                                    processed_tweets  \n",
      "0  woman complain cleaning house amp man always t...  \n",
      "1  boy dats cold tyga dwn bad cuffin dat hoe st p...  \n",
      "2       dawg ever fuck bitch start cry confused shit  \n",
      "3                                   look like tranny  \n",
      "4     shit hear might true might faker bitch told ya  \n",
      "5  shit blows claim faithful somebody still fucki...  \n",
      "6         sit hate another bitch got much shit going  \n",
      "7     cause tired big bitches coming us skinny girls  \n",
      "8              amp might get ya bitch back amp thats  \n",
      "9              hobbies include fighting mariam bitch  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "#extending the stopwords to include other words used in twitter such as retweet(rt) etc.\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\" , \"RT\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(tweet):  \n",
    "    # removal of extra spaces\n",
    "    regex_pat = re.compile(r'\\s+')\n",
    "    tweet_space = tweet.str.replace(regex_pat, ' ' , regex=True)\n",
    "\n",
    "    # removal of @name[mention]\n",
    "    regex_pat = re.compile(r'@[\\w\\-]+')\n",
    "    tweet_name = tweet_space.str.replace(regex_pat, '' , regex=True)\n",
    "\n",
    "    # removal of links[https://abc.com]\n",
    "    giant_url_regex =  re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "            r'[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    tweets = tweet_name.str.replace(giant_url_regex, '', regex=True)\n",
    "    \n",
    "    # removal of punctuations and numbers\n",
    "    punc_remove = tweets.str.replace(r\"[^a-zA-Z]\", \" \", regex=True)\n",
    "    # remove whitespace with a single space\n",
    "    newtweet=punc_remove.str.replace(r'\\s+', ' ', regex=True)\n",
    "    # remove leading and trailing whitespace\n",
    "    newtweet=newtweet.str.replace(r'^\\s+|\\s+?$','', regex=True)\n",
    "    # replace normal numbers with numbr\n",
    "    newtweet=newtweet.str.replace(r'\\d+(\\.\\d+)?','numbr', regex=True)\n",
    "    # removal of capitalization\n",
    "    tweet_lower = newtweet.str.lower()\n",
    "    \n",
    "    # tokenizing\n",
    "    tokenized_tweet = tweet_lower.apply(lambda x: x.split())\n",
    "    \n",
    "    # removal of stopwords\n",
    "    tokenized_tweet=  tokenized_tweet.apply(lambda x: [item for item in x if item not in stopwords])\n",
    "    \n",
    "    # stemming of the tweets\n",
    "   # tokenized_tweet = tokenized_tweet.apply(lambda x: [lemmatizer.lemmatize(i) for i in x])\n",
    "    \n",
    "    for i in range(len(tokenized_tweet)):\n",
    "        tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "        tweets_p= tokenized_tweet\n",
    "    \n",
    "    return tweets_p\n",
    "    \n",
    "processed_tweets = preprocess(df.tweet)   \n",
    "\n",
    "df['processed_tweets'] = processed_tweets\n",
    "print(df[[\"tweet\",\"processed_tweets\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8958307861596773\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.21      0.29       465\n",
      "           1       0.92      0.96      0.94      6335\n",
      "           2       0.85      0.84      0.84      1379\n",
      "\n",
      "    accuracy                           0.90      8179\n",
      "   macro avg       0.74      0.67      0.69      8179\n",
      "weighted avg       0.88      0.90      0.89      8179\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Assume 'df' is your DataFrame with the dataset\n",
    "X = df[['processed_tweets']]\n",
    "y = df['class']\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Convert text to numerical features using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer( max_features=10000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['processed_tweets'])\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val['processed_tweets'])\n",
    "\n",
    "X_train_final = X_train_tfidf\n",
    "X_val_final = X_val_tfidf\n",
    "# Train a model (e.g., RandomForestClassifier)\n",
    "clf =  LogisticRegression(C = 4, random_state= 42)\n",
    "clf.fit(X_train_final, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = clf.predict(X_val_final)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' #find best c parameter with cross validation\\nfrom sklearn.model_selection import GridSearchCV\\nparam_grid = {\\'C\\': [0.001, 0.01, 0, 0.1, 1, 10, 100]}\\ngrid_search = GridSearchCV(LogisticRegression(random_state=42 , max_iter= 10000), param_grid, cv=5 )\\ngrid_search.fit(X_train_final, y_train)\\n\\nbest_params = grid_search.best_params_\\nprint(f\"Best Hyperparameters: {best_params}\") '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" #find best c parameter with cross validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [0.001, 0.01, 0, 0.1, 1, 10, 100]}\n",
    "grid_search = GridSearchCV(LogisticRegression(random_state=42 , max_iter= 10000), param_grid, cv=5 )\n",
    "grid_search.fit(X_train_final, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Hyperparameters: {best_params}\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    women\n",
      "dtype: object\n",
      "The prediction for the sentence is: Offesnive\n"
     ]
    }
   ],
   "source": [
    "# Example sentence\n",
    "new_sentence = pd.Series('women')\n",
    "\n",
    "new_sentence = preprocess(new_sentence)\n",
    "print(new_sentence)\n",
    "\n",
    "\n",
    "# Convert the new sentence to TF-IDF features\n",
    "new_sentence_tfidf = tfidf_vectorizer.transform([new_sentence[0]])\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions for the new sentence\n",
    "prediction = clf.predict(new_sentence_tfidf)\n",
    "\n",
    "# Print the prediction\n",
    "classifciationType = ''\n",
    "if(prediction[0] == 0):\n",
    "    classifciationType = 'Hate Speech'\n",
    "elif(prediction[0] == 1): \n",
    "        classifciationType = 'Offesnive'\n",
    "elif(prediction[0]==2):\n",
    "        classifciationType = 'Neither'\n",
    "\n",
    "\n",
    "print(f\"The prediction for the sentence is: {classifciationType}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
